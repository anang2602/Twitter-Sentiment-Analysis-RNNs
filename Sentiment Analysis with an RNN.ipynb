{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis witn an RNN\n",
    "In this notebook, you'll implement a recurrent neural network that performs sentiment analysis.\n",
    "\n",
    "> Using an RNN rather than a strictly feedforward network is more acurate since we can include information about the sequence of words.\n",
    "\n",
    "Here we'll use a dataset of tweets, accompanied by sentiment labels: positive or negative\n",
    "\n",
    "<img src=\"assets/img1.png\" width=40%>\n",
    "\n",
    "## Network Architecture\n",
    "The architecture for this network is shown below\n",
    "\n",
    "<img src=\"assets/img2.png\" width=60%>\n",
    "\n",
    "> **First we'll pass in words to an embedding layer.** We need an embedding layer because we have tens of thousands of words, so we'll need a more efficient representation for our inout data than one-hot encoded vectors. You can actually train an embedding with the Skip-gram Word2Vec model and use those embeddings as input, here. However, it's good enough to just have an embedding layer and let the network learn a different embedding table on its own. _In this case, the embedding layer is for dimensionality reduction, rather than for learning semantic representation._\n",
    "\n",
    "> **After Input words are passed to an embedding layer, the new embeddings will be passed to LSTM cells.** The LSTM cells will add _recurrent_ connections to the network and give us the ability to include information about the sequence of words in the tweets data.\n",
    "\n",
    "> **Finally, the LSTM outputs will go to a sigmoid output layer.** We're using a sigmoid function because positive and negative = 1 and 0, respectively, and sigmoid will output predicted, sentiment values between 0-1\n",
    "\n",
    "We don't care about sigmoid outputs except for the **very last one** we can ignore the rest. We'll calculate the loss by comparing the output at the last time step and the training label (pos or neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in and visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hashtags</th>\n",
       "      <th>username</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>tweet_url</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>text_html</th>\n",
       "      <th>links</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Des B</td>\n",
       "      <td>1.100000e+18</td>\n",
       "      <td>/descanto/status/1098177427241607168</td>\n",
       "      <td>2/20/19 11:07</td>\n",
       "      <td>Cinta itu adalah jiwa. Antara cinta sejati ...</td>\n",
       "      <td>&lt;p class=\"TweetTextSize js-tweet-text tweet-te...</td>\n",
       "      <td>['https://www.instagram.com/p/BuGfDr1FKJn/?utm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>yeni susanti</td>\n",
       "      <td>1.100000e+18</td>\n",
       "      <td>/yenysusanty/status/1097091130422890497</td>\n",
       "      <td>2/17/19 11:11</td>\n",
       "      <td>Selalu salut sama @emakblogersolo yg gak henti...</td>\n",
       "      <td>&lt;p class=\"TweetTextSize js-tweet-text tweet-te...</td>\n",
       "      <td>['https://www.instagram.com/p/Bt-xJLygYbh/?utm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>latifah kusuma</td>\n",
       "      <td>1.100000e+18</td>\n",
       "      <td>/latifahkusuma7/status/1095150154439483392</td>\n",
       "      <td>2/12/19 02:38</td>\n",
       "      <td>Ternyata film_okb Mengandung Kisah Nyata Penul...</td>\n",
       "      <td>&lt;p class=\"TweetTextSize js-tweet-text tweet-te...</td>\n",
       "      <td>['https://www.instagram.com/p/Btw-fVygrMc/?utm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>['_Ù__ä_ç___ç_____µ__µ__Ù', 'yogyakarta...</td>\n",
       "      <td>Imam Munandar</td>\n",
       "      <td>1.090000e+18</td>\n",
       "      <td>/nandar_euy/status/1094458868522934272</td>\n",
       "      <td>2/10/19 04:51</td>\n",
       "      <td>Boleh Miskin Asal Bahagia. \\rAsal tau aja cara...</td>\n",
       "      <td>&lt;p class=\"TweetTextSize js-tweet-text tweet-te...</td>\n",
       "      <td>['https://www.instagram.com/p/BtsEIXsFzI1/?utm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>['dolanindie', 'sisijogja', 'infojogja', 'expl...</td>\n",
       "      <td>Mas Titis Waelah</td>\n",
       "      <td>1.090000e+18</td>\n",
       "      <td>/maz_Titiz/status/1094396002167996418</td>\n",
       "      <td>2/10/19 00:41</td>\n",
       "      <td>Janganlah engkau MENGARUNGI LAUTAN, percuma de...</td>\n",
       "      <td>&lt;p class=\"TweetTextSize js-tweet-text tweet-te...</td>\n",
       "      <td>['https://www.instagram.com/p/Btrnis6lGF9/?utm...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            hashtags          username  \\\n",
       "0                                                 []             Des B   \n",
       "1                                                 []      yeni susanti   \n",
       "2                                                 []    latifah kusuma   \n",
       "3  ['_Ù__ä_ç___ç_____µ__µ__Ù', 'yogyakarta...     Imam Munandar   \n",
       "4  ['dolanindie', 'sisijogja', 'infojogja', 'expl...  Mas Titis Waelah   \n",
       "\n",
       "       tweet_id                                   tweet_url      timestamp  \\\n",
       "0  1.100000e+18        /descanto/status/1098177427241607168  2/20/19 11:07   \n",
       "1  1.100000e+18     /yenysusanty/status/1097091130422890497  2/17/19 11:11   \n",
       "2  1.100000e+18  /latifahkusuma7/status/1095150154439483392  2/12/19 02:38   \n",
       "3  1.090000e+18      /nandar_euy/status/1094458868522934272  2/10/19 04:51   \n",
       "4  1.090000e+18       /maz_Titiz/status/1094396002167996418  2/10/19 00:41   \n",
       "\n",
       "                                                text  \\\n",
       "0  Cinta itu adalah jiwa. Antara cinta sejati ...   \n",
       "1  Selalu salut sama @emakblogersolo yg gak henti...   \n",
       "2  Ternyata film_okb Mengandung Kisah Nyata Penul...   \n",
       "3  Boleh Miskin Asal Bahagia. \\rAsal tau aja cara...   \n",
       "4  Janganlah engkau MENGARUNGI LAUTAN, percuma de...   \n",
       "\n",
       "                                           text_html  \\\n",
       "0  <p class=\"TweetTextSize js-tweet-text tweet-te...   \n",
       "1  <p class=\"TweetTextSize js-tweet-text tweet-te...   \n",
       "2  <p class=\"TweetTextSize js-tweet-text tweet-te...   \n",
       "3  <p class=\"TweetTextSize js-tweet-text tweet-te...   \n",
       "4  <p class=\"TweetTextSize js-tweet-text tweet-te...   \n",
       "\n",
       "                                               links  label  \n",
       "0  ['https://www.instagram.com/p/BuGfDr1FKJn/?utm...      0  \n",
       "1  ['https://www.instagram.com/p/Bt-xJLygYbh/?utm...      0  \n",
       "2  ['https://www.instagram.com/p/Btw-fVygrMc/?utm...      0  \n",
       "3  ['https://www.instagram.com/p/BtsEIXsFzI1/?utm...      0  \n",
       "4  ['https://www.instagram.com/p/Btrnis6lGF9/?utm...      0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read data\n",
    "# TO-DO fill the filename with your file path \n",
    "filename = \"beras_or_miskin.csv\"\n",
    "df = pd.read_csv(filename, encoding=\"ISO-8859-1\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we drop the columns that we don't need for specific purpose of sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['hashtags', 'username', 'tweet_id', 'tweet_url', 'timestamp', 'text_html','links'], axis =1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop duplicates row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.drop_duplicates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3207, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Cinta itu adalah jiwa. Antara cinta sejati ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Selalu salut sama @emakblogersolo yg gak henti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Ternyata film_okb Mengandung Kisah Nyata Penul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Boleh Miskin Asal Bahagia. \\rAsal tau aja cara...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Janganlah engkau MENGARUNGI LAUTAN, percuma de...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  Cinta itu adalah jiwa. Antara cinta sejati ...      0\n",
       "1  Selalu salut sama @emakblogersolo yg gak henti...      0\n",
       "2  Ternyata film_okb Mengandung Kisah Nyata Penul...      0\n",
       "3  Boleh Miskin Asal Bahagia. \\rAsal tau aja cara...      0\n",
       "4  Janganlah engkau MENGARUNGI LAUTAN, percuma de...      0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "As a way of snity check, let's look at the length of the string in the text column in each entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pre_clean_len'] = [len(t) for t in df.text.map(str)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot pre_clean_len with box plot, so that we can see the overall distribution of length of strings in each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.boxplot(df.pre_clean_len)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above box plot, some of tweets are way more than 140 characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>pre_clean_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Cinta itu adalah jiwa. Antara cinta sejati ...</td>\n",
       "      <td>0</td>\n",
       "      <td>285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Selalu salut sama @emakblogersolo yg gak henti...</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Ternyata film_okb Mengandung Kisah Nyata Penul...</td>\n",
       "      <td>0</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Boleh Miskin Asal Bahagia. \\rAsal tau aja cara...</td>\n",
       "      <td>0</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Janganlah engkau MENGARUNGI LAUTAN, percuma de...</td>\n",
       "      <td>0</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Sudahkah Anda memiliki tabungan di Akhirat ?? ...</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>Segelas Beras untuk Berdua\\r\"Untuk kami berdua...</td>\n",
       "      <td>0</td>\n",
       "      <td>276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>George Orwell, Bagaimana Si Miskin Mati, esai,...</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>Pergi krn tugas pulang karena beras @ JW Marri...</td>\n",
       "      <td>0</td>\n",
       "      <td>161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>_Hidup itu dinikmati_\\r\\r\"hidup itu dinikmati\"...</td>\n",
       "      <td>0</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  label  pre_clean_len\n",
       "0   Cinta itu adalah jiwa. Antara cinta sejati ...      0            285\n",
       "1   Selalu salut sama @emakblogersolo yg gak henti...      0            284\n",
       "2   Ternyata film_okb Mengandung Kisah Nyata Penul...      0            282\n",
       "3   Boleh Miskin Asal Bahagia. \\rAsal tau aja cara...      0            293\n",
       "4   Janganlah engkau MENGARUNGI LAUTAN, percuma de...      0            283\n",
       "5   Sudahkah Anda memiliki tabungan di Akhirat ?? ...      0            277\n",
       "12  Segelas Beras untuk Berdua\\r\"Untuk kami berdua...      0            276\n",
       "13  George Orwell, Bagaimana Si Miskin Mati, esai,...      0            277\n",
       "14  Pergi krn tugas pulang karena beras @ JW Marri...      0            161\n",
       "15  _Hidup itu dinikmati_\\r\\r\"hidup itu dinikmati\"...      0            283"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.pre_clean_len > 140].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dealing with '@'mention\n",
    "this information doesn't add value to build sentiment analysis model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gpp kok asal jangan sampe berdebu...\\r\\rMbak @lrstdw_  mau ikutan dibahagiain pake gombalan orang miskin atau pantun orang kolong jembatan Gak ?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[1907]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gpp kok asal jangan sampe berdebu...\\r\\rMbak _  mau ikutan dibahagiain pake gombalan orang miskin atau pantun orang kolong jembatan Gak ?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.sub(r'@[A-Za-z0-9]+','', df.text[1907])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with URL links\n",
    "same with @mention, for sentiment analysis purpose, this can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x8a\\x97\\x93Cinta itu adalah jiwa. Antara cinta sejati dan jiwa tak dapat dipisahkan, cintapun merdeka sebagaimana jiwa. Ia tidak membedakan di antara derajat dan bangsa, di antara kaya dan miskin,\\x8a\\x97_ https://www.instagram.com/p/BuGfDr1FKJn/?utm_source=ig_twitter_share&igshid=d9qqr424pl5wÎ¾\\x8a\\x97_'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x8a\\x97\\x93Cinta itu adalah jiwa. Antara cinta sejati dan jiwa tak dapat dipisahkan, cintapun merdeka sebagaimana jiwa. Ia tidak membedakan di antara derajat dan bangsa, di antara kaya dan miskin,\\x8a\\x97_ '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"https\\S+\",'',df.text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dealing with hashtag / numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tolong ya kalean semua sadar, bela keluarga kalean dulu. berjuangan buat buat keluarga biar ga ngeluh miskin terus . orang yg kalian bela skrg mungkin lg ena ena dan ketawa. tai #TangkapPRABOWO'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[485]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tolong ya kalean semua sadar  bela keluarga kalean dulu  berjuangan buat buat keluarga biar ga ngeluh miskin terus   orang yg kalian bela skrg mungkin lg ena ena dan ketawa  tai  TangkapPRABOWO'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"[^a-zA-Z]\", \" \", df.text[485])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining data cleaning function\n",
    "With above data cleaning task, we will define data cleaning function and then will be applied to the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jauh jauh harga susu bayi harga beras kg aja kayaknya gak ngerti itu orang',\n",
       " 'dari pengguna twitter instagram facebook menyerukan gerakan menikahlah dan kau akan hidup enak bahagia sentausa yg mana mereka sendiri aja gak tau mumetnya bayar tagihan listrik bayar iuran sampah lingkungan jatah beli beras stok sabun cuci belum lagi dadakan loro',\n",
       " 'pk tepung beras rosbren ya',\n",
       " 'ketika diskon lebaran dan isi dompet tidak sehati berasa orang paling miskin dimuka bumi',\n",
       " 'buka puasa pake beras kencur enak jugak ya',\n",
       " 'berorganisasi membuatku bertambah miskin',\n",
       " 'sekarang banyak insan yang resah untuk melangkah di ladang sendiri tidak lain di karenakan premanism juga segolongan yang selalu mementingkan diri sendiri sangat ironis sekali sungguh miskin moral di tanah beribu budaya',\n",
       " 'sudah mulai kelihatan diskonan hari raya tp maaf ngga tergoda gw gimana mau tergoda sadar miskin duluan gw anj gamampu beli',\n",
       " 'operasi kelamin pake beras trans gendar',\n",
       " 'aduh gamau aku pake white pants aku miskin dan jorok']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r\"https\\S+\"\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "\n",
    "def tweet_cleaner(text):\n",
    "    stripped = re.sub(combined_pat, '', text)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", stripped)\n",
    "    lower_case = letters_only.lower()\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "\n",
    "testing = df.text[50:60]\n",
    "\n",
    "test_result = []\n",
    "for t in testing:\n",
    "    test_result.append(tweet_cleaner(t))\n",
    "test_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in df.text:\n",
    "    df.loc[(df.text == t), 'text'] = tweet_cleaner(str(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Cleaned data as csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>cinta itu adalah jiwa antara cinta sejati dan ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>selalu salut sama yg gak henti nya memfasilita...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ternyata film okb mengandung kisah nyata penul...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>boleh miskin asal bahagia asal tau aja caranya...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>janganlah engkau mengarungi lautan percuma deh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  cinta itu adalah jiwa antara cinta sejati dan ...      0\n",
       "1  selalu salut sama yg gak henti nya memfasilita...      0\n",
       "2  ternyata film okb mengandung kisah nyata penul...      0\n",
       "3  boleh miskin asal bahagia asal tau aja caranya...      0\n",
       "4  janganlah engkau mengarungi lautan percuma deh...      0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TO-DO change the filename\n",
    "df.drop('pre_clean_len', axis=1).to_csv('clean_beras_or_miskin.csv', encoding='utf-8', index=False)\n",
    "clean_df = pd.read_csv('clean_beras_or_miskin.csv')\n",
    "clean_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data pre-processing\n",
    "The first step when building a neural network model is getting your data into the proper form to feed into the network. Since we're using embedding layers, we'll need to encode each word with an integer.\n",
    "\n",
    "First, let remove all punctuation. Then get all the text without newlines and split it into individual words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "all_text = '\\n'.join([c for c in clean_df.text.map(str) if c not in punctuation])\n",
    "\n",
    "#split by new lines and spaces\n",
    "tweets_split = all_text.split('\\n')\n",
    "all_text = ' '.join(tweets_split)\n",
    "\n",
    "# create a list of words\n",
    "words = all_text.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3207\n",
      "cinta itu adalah jiwa antara cinta sejati dan jiwa tak dapat dipisahkan cintapun merdeka sebagaimana\n",
      "46233\n"
     ]
    }
   ],
   "source": [
    "print(len(tweets_split))\n",
    "print(all_text[:100])\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the words\n",
    "The embedding lookup requires that we pass in integers to out network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Then we can convert each of our tweets into integers so they can be passed into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#build dictionary that maps words to integers\n",
    "counts = Counter(words)\n",
    "vocab = sorted(counts, key=counts.get, reverse=True)\n",
    "vocab_to_int = {word: ii for ii, word in enumerate(vocab, 1)}\n",
    "\n",
    "# use the dict to tokenize each tweets in tweets_split\n",
    "## store the tokenized tweets in tweets_ints\n",
    "tweets_ints = []\n",
    "for tweet in tweets_split:\n",
    "    tweets_ints.append([vocab_to_int[word] for word in tweet.split()])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print out the number of unique words in our vocabulary and the contents of the first, tokenized review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  10057\n",
      "\n",
      "Tokenized review: \n",
      " [[298, 11, 66, 123, 436, 298, 2412, 7, 123, 54, 376, 3827, 3828, 1769, 3829, 123, 569, 33, 1424, 4, 436, 2413, 7, 1179, 4, 436, 9, 7, 1]]\n"
     ]
    }
   ],
   "source": [
    "# stats about vocabulary\n",
    "print('Unique words: ', len((vocab_to_int)))\n",
    "print()\n",
    "\n",
    "# print tokens in first review\n",
    "print(\"Tokenized review: \\n\", tweets_ints[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Outliers\n",
    "As an additional pre-processing step, we want to make sure taht our reviews are in good shape for standard processing. That is, our network will expect a standard input text size, and so, we'll want to shape our tweets into a specific length. We'll approach this task in two main steps:\n",
    "\n",
    "1. Getting rid of extremely long or short tweets; the outliers\n",
    "2. Padding/truncating the remaining data so that we have reviews of the same length.\n",
    "\n",
    "Before we pad our tweets text, we should check for tweets of extremely short or long lengths; outliers that may mess with our training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-length tweets: 0\n",
      "Maximum tweets length: 67\n"
     ]
    }
   ],
   "source": [
    "# outlier tweets stats\n",
    "tweets_lens = Counter([len(x) for x in tweets_ints])\n",
    "print(\"Zero-length tweets: {}\".format(tweets_lens[0]))\n",
    "print(\"Maximum tweets length: {}\".format(max(tweets_lens)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have tweets with zero length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding sequences\n",
    "To deal with short and very long tweets, we'll pad or truncate all our tweets to a specific length. For tweets shorter than some `seq_length`, we'll pad with 0s. For tweets longer than `seq_length`, we can truncate them to the first `seq_length` words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_features(tweets_ints, seq_length):\n",
    "    '''return features of tweets_ints, where each tweets\n",
    "    is padded with 0's or truncated to the input seq_length\n",
    "    '''\n",
    "    # getting the correct rows x cols shape\n",
    "    features = np.zeros((len(tweets_ints), seq_length), dtype=int)\n",
    "    \n",
    "    for i, row in enumerate(tweets_ints):\n",
    "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
    "    \n",
    "    return features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0  134 3830   29    6   30 3831   47 2414  783    1  494\n",
      "  703 3832  146   64   13 1770 2415  626  570    4 3833   35   22 1771]\n"
     ]
    }
   ],
   "source": [
    "seq_length= 140\n",
    "\n",
    "features = pad_features(tweets_ints, seq_length=seq_length)\n",
    "\n",
    "## Test Statement ##\n",
    "assert len(features)==len(tweets_ints), \"Your features should have as many rows as tweets.\"\n",
    "assert len(features[0])==seq_length, \"Each feature row should contain seq_length values.\"\n",
    "\n",
    "print(features[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, validation, Test\n",
    "With our data in nice shape, we'll split it into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(2565, 140) \n",
      "Validation set: \t(321, 140) \n",
      "Test set: \t\t(321, 140)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data\n",
    "## (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*split_frac)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = clean_df.label[:split_idx], clean_df.label[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of your resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loaders and Batching\n",
    "After creating training, test and validation data, we can create DataLoaders for this data\n",
    "1. Create a known format for accessing our data, using TensorDataset which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n",
    "2. Create DataLoaders and batch our training, validation, and test Tensor adtasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(np.array(train_x)), torch.from_numpy(np.array(train_y)))\n",
    "valid_data = TensorDataset(torch.from_numpy(np.array(val_x)), torch.from_numpy(np.array(val_y)))\n",
    "test_data = TensorDataset(torch.from_numpy(np.array(test_x)), torch.from_numpy(np.array(test_y)))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 50\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size, drop_last=True)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input size:  torch.Size([50, 200])\n",
      "Sample input: \n",
      " tensor([[   0,    0,    0,  ...,   96, 7265,    1],\n",
      "        [   0,    0,    0,  ...,    1,   17,  860],\n",
      "        [   0,    0,    0,  ...,    5,   17,  423],\n",
      "        ...,\n",
      "        [   0,    0,    0,  ...,    0,  729,    2],\n",
      "        [   0,    0,    0,  ...,   12,    5, 6763],\n",
      "        [   0,    0,    0,  ...,    1,  151, 2298]])\n",
      "\n",
      "Sample label size:  torch.Size([50])\n",
      "Sample label: \n",
      " tensor([0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
      "        0, 0])\n"
     ]
    }
   ],
   "source": [
    "# obtain one batch of training data\n",
    "dataiter = iter(train_loader)\n",
    "sample_x, sample_y = dataiter.next()\n",
    "\n",
    "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
    "print('Sample input: \\n', sample_x)\n",
    "print()\n",
    "print('Sample label size: ', sample_y.size()) # batch_size\n",
    "print('Sample label: \\n', sample_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Sentiment Network with PyTorch\n",
    "\n",
    "Below is where we'll define the network.\n",
    "\n",
    "<img src=\"assets/img2.png\" width=50%>\n",
    "\n",
    "The layers are as follows:\n",
    "1. An [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) that converts our word tokens (integers) into embeddings of a specific size.\n",
    "2. An [LSTM layer](https://pytorch.org/docs/stable/nn.html#lstm) defined by a hidden_state size and number of layers\n",
    "3. A fully-connected output layer that maps the LSTM layer outputs to a desired output_size\n",
    "4. A sigmoid activation layer which turns all outputs into a value 0-1; return **only the last sigmoid output** as the output of this network.\n",
    "\n",
    "### The Embedding Layer\n",
    "We need to add an [embedding layer](https://pytorch.org/docs/stable/nn.html#embedding) because there are 10000+ words in our vocabulary. It is massively inefficient to one-hot encode that many classes. So, instead of one-hot encoding, we can have an embedding layer and use that layer as a lookup table. You could train an embedding layer using Word2Vec, then load it here. But, it's fine to just make a new layer, using it for only dimensionality reduction, and let the network learn the weights.\n",
    "\n",
    "### The LSTM Layer(s)\n",
    "\n",
    "We'll create an [LSTM](https://pytorch.org/docs/stable/nn.html#lstm) to use in our recurrent network, which takes in an input_size, a hidden_dim, a number of layers, a dropout probability (for dropout between multiple layers), and a batch_first parameter.\n",
    "\n",
    "Most of the time, you're network will have better performance with more layers; between 2-3. Adding more layers allows the network to learn really complex relationships. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_dim, num_layers=n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear and sigmoid layers\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        self.sig = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        x = x.long()\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "    \n",
    "        # stack up lstm outputs\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # dropout and fully-connected layer\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)\n",
    "        # sigmoid function\n",
    "        sig_out = self.sig(out)\n",
    "        \n",
    "        # reshape to be batch_size first\n",
    "        sig_out = sig_out.view(batch_size, -1)\n",
    "        sig_out = sig_out[:, -1] # get last batch of labels\n",
    "        \n",
    "        # return last sigmoid output and hidden state\n",
    "        return sig_out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the network\n",
    "\n",
    "Here, we'll instantiate the network. First up, defining the hyperparameters.\n",
    "\n",
    "* `vocab_size`: Size of our vocabulary or the range of values for our input, word tokens.\n",
    "* `output_size`: Size of our desired output; the number of class scores we want to output (pos/neg).\n",
    "* `embedding_dim`: Number of columns in the embedding lookup table; size of our embeddings.\n",
    "* `hidden_dim`: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
    "* `n_layers`: Number of LSTM layers in the network. Typically between 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(10058, 100)\n",
      "  (lstm): LSTM(100, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (sig): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(vocab_to_int) + 1 # +1 for the 0 padding + our word tokens\n",
    "output_size = 1\n",
    "embedding_dim = 100\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "* `lr`: Learning rate for our optimizer.\n",
    "* `epochs`: Number of times to iterate through the training dataset.\n",
    "* `clip`: The maximum gradient value to clip at (to prevent exploding gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/4... Step: 10... Loss: 0.682606... Val Loss: 1.308280\n",
      "Epoch: 1/4... Step: 20... Loss: 0.537052... Val Loss: 1.253924\n",
      "Epoch: 1/4... Step: 30... Loss: 0.494214... Val Loss: 1.193625\n",
      "Epoch: 1/4... Step: 40... Loss: 0.570617... Val Loss: 1.182104\n",
      "Epoch: 1/4... Step: 50... Loss: 0.515990... Val Loss: 1.338417\n",
      "Epoch: 2/4... Step: 60... Loss: 0.427719... Val Loss: 1.435523\n",
      "Epoch: 2/4... Step: 70... Loss: 0.407219... Val Loss: 1.131366\n",
      "Epoch: 2/4... Step: 80... Loss: 0.456682... Val Loss: 1.341394\n",
      "Epoch: 2/4... Step: 90... Loss: 0.587985... Val Loss: 1.421694\n",
      "Epoch: 2/4... Step: 100... Loss: 0.570724... Val Loss: 1.385773\n",
      "Epoch: 3/4... Step: 110... Loss: 0.363232... Val Loss: 1.366840\n",
      "Epoch: 3/4... Step: 120... Loss: 0.588041... Val Loss: 1.358652\n",
      "Epoch: 3/4... Step: 130... Loss: 0.294500... Val Loss: 1.925881\n",
      "Epoch: 3/4... Step: 140... Loss: 0.328632... Val Loss: 1.562441\n",
      "Epoch: 3/4... Step: 150... Loss: 0.428853... Val Loss: 1.777950\n",
      "Epoch: 4/4... Step: 160... Loss: 0.142172... Val Loss: 2.367231\n",
      "Epoch: 4/4... Step: 170... Loss: 0.285295... Val Loss: 1.836337\n",
      "Epoch: 4/4... Step: 180... Loss: 0.344820... Val Loss: 2.819110\n",
      "Epoch: 4/4... Step: 190... Loss: 0.242898... Val Loss: 1.933849\n",
      "Epoch: 4/4... Step: 200... Loss: 0.222413... Val Loss: 1.780086\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "\n",
    "epochs = 4 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 10\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        \n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output.squeeze(), labels.float())\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output.squeeze(), labels.float())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Testing\n",
    "\n",
    "There are a few ways to test your network.\n",
    "- **Test data performance :** First, we'll see how our trained model performs on all our defined test_data, above. We'll calculate the average loss and accuracy over the test data.\n",
    "- **Inference on user-generated data:** Second, We'll see if we can input just one example review at a time (without a label), and see what the trained model predicts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.179\n",
      "Test accuracy: 0.268\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    pred = torch.round(output.squeeze())  # rounds to the nearest integer\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
